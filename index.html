<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deployed Voice Tutor (ASR & NLP Integration)</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f0f8ff;
        }

        .card {
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.1);
        }

        #recordButton.recording {
            background-color: #ef4444;
            animation: pulse 1s infinite;
        }

        @keyframes pulse {
            0% {
                transform: scale(1);
            }

            50% {
                transform: scale(1.05);
                box-shadow: 0 0 0 10px rgba(239, 68, 68, 0.4);
            }

            100% {
                transform: scale(1);
            }
        }

        .intent-tag {
            display: inline-block;
            padding: 2px 8px;
            margin-left: 10px;
            border-radius: 9999px;
            font-size: 0.75rem;
            font-weight: 600;
        }
    </style>
</head>

<body class="min-h-screen flex items-center justify-center p-4">

    <div id="app-container" class="w-full max-w-lg bg-white rounded-xl card p-6 md:p-8">
        <h1 class="text-3xl font-extrabold text-center text-blue-800 mb-2">
            ðŸš€ Deployed Adaptive Tutor
        </h1>
        <p class="text-center text-sm text-gray-600 mb-6">
            Voice command recognized by **your $\mathbf{>80\%}$ CNN model**.
        </p>

        <!-- Recording and UI Status Area -->
        <div class="flex flex-col items-center space-y-6">
            <button id="recordButton" class="w-24 h-24 rounded-full bg-green-500 text-white flex items-center justify-center 
                           text-3xl hover:bg-green-600 focus:outline-none focus:ring-4 focus:ring-green-300">
                ðŸŽ¤
            </button>

            <p id="statusMessage" class="text-lg font-semibold text-gray-700">Ready. Start Python server first!</p>

            <div id="loadingIndicator" class="hidden flex items-center">
                <!-- SVG Spinner -->
                <svg class="animate-spin h-6 w-6 text-blue-500 mr-3" xmlns="http://www.w3.org/2000/svg" fill="none"
                    viewBox="0 0 24 24">
                    <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                    <path class="opacity-75" fill="currentColor"
                        d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z">
                    </path>
                </svg>
                <span class="text-blue-500">Processing via **Local ML Server**...</span>
            </div>
        </div>

        <!-- Conversation Output -->
        <div class="mt-8 pt-6 border-t border-gray-200">
            <h2 class="text-xl font-bold text-gray-700 mb-4">Pipeline Output</h2>

            <div class="space-y-4">
                <!-- ASR Transcription Box -->
                <div class="bg-indigo-50 p-4 rounded-lg">
                    <p class="text-xs font-medium text-indigo-600 mb-1">
                        ASR Transcription (from **YOUR CNN**):
                        <span id="intent-display" class="intent-tag bg-blue-200 text-blue-800"></span>
                    </p>
                    <p id="user-transcription" class="text-gray-800 font-bold uppercase italic"></p>
                </div>

                <!-- Tutor Response Box -->
                <div class="bg-purple-50 p-4 rounded-lg min-h-[100px]">
                    <p class="text-xs font-medium text-purple-600 mb-1">Tutor Response (via Gemini LLM):</p>
                    <p id="tutor-response" class="text-gray-800"></p>
                </div>
            </div>
        </div>
    </div>

    <script>
        // --- CONFIGURATION ---
        const GEMINI_API_KEY = "YOUR_PASTED_API_KEY_GOES_HERE";
        const GEMINI_API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=${GEMINI_API_KEY}`;

        // Endpoints point to the local Flask server defined in server.py
        const LOCAL_ASR_ENDPOINT = 'http://127.0.0.1:5000/transcribe';
        const LOCAL_NLP_ENDPOINT = 'http://127.0.0.1:5000/classify_intent';

        // --- LLM Persona (Adaptive Tutor) ---
        const systemInstruction = {
            parts: [{
                text: "You are a friendly, encouraging, and patient virtual tutor for young learners (ages 5-10). Answer concisely. Base your response on the user's intent. If the intent is 'STOP', confirm the lesson is paused. If 'GREETING', respond politely. If 'NUMERACY', give a small math challenge."
            }]
        };

        // --- DOM and State Variables ---
        const recordButton = document.getElementById('recordButton');
        const statusMessage = document.getElementById('statusMessage');
        const loadingIndicator = document.getElementById('loadingIndicator');
        const userTranscriptionElement = document.getElementById('user-transcription');
        const tutorResponseElement = document.getElementById('tutor-response');
        const intentDisplayElement = document.getElementById('intent-display');

        let mediaRecorder;
        let isRecording = false;

        function arrayBufferToBase64(buffer) {
            let binary = '';
            const bytes = new Uint8Array(buffer);
            const len = bytes.byteLength;
            for (let i = 0; i < len; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return btoa(binary);
        }

        // --- Step 1: Handle Recording ---
        recordButton.onclick = () => {
            if (isRecording) { stopRecording(); }
            else { startRecording(); }
        };

        function startRecording() {
            // Clears UI
            userTranscriptionElement.textContent = '';
            tutorResponseElement.textContent = '';
            intentDisplayElement.textContent = '';
            intentDisplayElement.className = 'intent-tag';

            navigator.mediaDevices.getUserMedia({ audio: true })
                .then(stream => {
                    // Record only 1 second of audio (matching ASR CNN training duration)
                    mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm; codecs=opus' });
                    let audioChunks = [];
                    mediaRecorder.start();
                    isRecording = true;

                    recordButton.classList.add('recording');
                    recordButton.textContent = 'ðŸ›‘';
                    statusMessage.textContent = 'Listening... (Speak a command like YES, NO, UP, DOWN, or STOP)';

                    mediaRecorder.ondataavailable = event => { audioChunks.push(event.data); };

                    // Auto-stop recording after 1 second
                    setTimeout(() => {
                        if (isRecording) mediaRecorder.stop();
                    }, 1000);

                    mediaRecorder.onstop = async () => {
                        stream.getTracks().forEach(track => track.stop());
                        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                        await processAudio(audioBlob);
                    };
                })
                .catch(err => {
                    statusMessage.textContent = `Error: Microphone access denied or browser incompatibility.`;
                    console.error("Microphone error:", err);
                });
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                isRecording = false;
                recordButton.classList.remove('recording');
                recordButton.textContent = 'ðŸŽ¤';
                statusMessage.textContent = 'Processing...';
                loadingIndicator.classList.remove('hidden');
            }
        }

        // --- Utility to style the intent tag ---
        function styleIntentTag(intent) {
            let color = 'bg-gray-300 text-gray-800';
            if (intent === 'NUMERACY') color = 'bg-green-200 text-green-800';
            else if (intent === 'LITERACY') color = 'bg-yellow-200 text-yellow-800';
            else if (intent === 'GREETING') color = 'bg-blue-200 text-blue-800';
            else if (intent === 'CONFUSION') color = 'bg-orange-200 text-orange-800';
            else if (intent === 'SHIFT') color = 'bg-purple-200 text-purple-800';

            intentDisplayElement.className = `intent-tag ${color}`;
            intentDisplayElement.textContent = intent;
        }


        // --- Step 2, 3, & 4: Full Pipeline Execution ---
        async function processAudio(audioBlob) {
            let transcription = "ERROR";
            let intent = "UNKNOWN";

            try {
                const arrayBuffer = await audioBlob.arrayBuffer();
                const base64Audio = arrayBufferToBase64(arrayBuffer);

                // --- 1. ASR (LOCAL CNN MODEL) ---
                statusMessage.textContent = '1/3: Running ASR on Local CNN...';
                const asrResponse = await fetch(LOCAL_ASR_ENDPOINT, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ audio: base64Audio })
                });

                // Check for connection failure (e.g., server not running)
                if (!asrResponse.ok) {
                    const errorText = await asrResponse.text();
                    throw new Error(`ASR Server Failed. Response Status: ${asrResponse.status}. Error: ${errorText.substring(0, 50)}...`);
                }

                const asrResult = await asrResponse.json();
                transcription = asrResult.transcription || "COULD NOT TRANSCRIBE";
                userTranscriptionElement.textContent = transcription.toUpperCase();

                // --- 2. NLP (LOCAL DISTILBERT MODEL) ---
                statusMessage.textContent = '2/3: Classifying Intent using DistilBERT...';
                const nlpResponse = await fetch(LOCAL_NLP_ENDPOINT, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ text: transcription })
                });

                if (!nlpResponse.ok) throw new Error(`NLP Server Failed. Check server.py console.`);

                const nlpResult = await nlpResponse.json();
                intent = nlpResult.intent;
                styleIntentTag(intent); // Apply color and text to the intent tag

                // --- 3. LLM GENERATION (GEMINI) ---
                statusMessage.textContent = '3/3: Generating Adaptive Response with Gemini...';
                const finalPrompt = `The child's classified intent is: ${intent}. The command spoken was: ${transcription}. Provide an adaptive tutor response based on this.`;

                const llmResult = await callGeminiAPI(finalPrompt);
                tutorResponseElement.textContent = llmResult.generatedText;

            } catch (error) {
                tutorResponseElement.textContent = `CRITICAL ERROR: ${error.message}. Ensure the server is active, models are loaded, and API key is correct.`;
                console.error("Full Processing Error:", error);
            } finally {
                loadingIndicator.classList.add('hidden');
                statusMessage.textContent = 'Ready. Speak a new command!';
            }
        }

        // --- Gemini API Handler (Text Only) ---
        async function callGeminiAPI(textPrompt) {
            if (GEMINI_API_KEY === "YOUR_PASTED_API_KEY_GOES_HERE") {
                return { generatedText: "ERROR: Please replace 'YOUR_PASTED_API_KEY_GOES_HERE' with your actual Google API Key." };
            }

            const payload = {
                contents: [{ role: "user", parts: [{ text: textPrompt }] }],
                systemInstruction: systemInstruction,
            };

            const response = await fetch(GEMINI_API_URL, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
            });

            const result = await response.json();
            const generatedText = result.candidates?.[0]?.content?.parts?.[0]?.text || "Gemini could not generate a response.";

            return { generatedText };
        }
    </script>
</body>

</html>